{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 1000, 8)           80000     \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 8001      \n",
      "=================================================================\n",
      "Total params: 88,001\n",
      "Trainable params: 88,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 69us/sample - loss: 0.5430 - binary_accuracy: 0.7336 - val_loss: 0.3527 - val_binary_accuracy: 0.8614\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 58us/sample - loss: 0.2771 - binary_accuracy: 0.8921 - val_loss: 0.2861 - val_binary_accuracy: 0.8788\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 56us/sample - loss: 0.2150 - binary_accuracy: 0.9163 - val_loss: 0.2722 - val_binary_accuracy: 0.8868\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 57us/sample - loss: 0.1817 - binary_accuracy: 0.9310 - val_loss: 0.2708 - val_binary_accuracy: 0.8940\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 56us/sample - loss: 0.1595 - binary_accuracy: 0.9401 - val_loss: 0.2751 - val_binary_accuracy: 0.8956\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 57us/sample - loss: 0.1412 - binary_accuracy: 0.9480 - val_loss: 0.2966 - val_binary_accuracy: 0.8892\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 57us/sample - loss: 0.1252 - binary_accuracy: 0.9550 - val_loss: 0.3020 - val_binary_accuracy: 0.8902\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 58us/sample - loss: 0.1116 - binary_accuracy: 0.9607 - val_loss: 0.3117 - val_binary_accuracy: 0.8908\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 58us/sample - loss: 0.0991 - binary_accuracy: 0.9663 - val_loss: 0.3266 - val_binary_accuracy: 0.8876\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 58us/sample - loss: 0.0877 - binary_accuracy: 0.9707 - val_loss: 0.3384 - val_binary_accuracy: 0.8896\n",
      "CPU times: user 29.9 s, sys: 3.07 s, total: 32.9 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "# Embedding 층의 객체 생성하기\n",
    "\n",
    "embedding_layer = Embedding(1000, 64)\n",
    "    # 임베딩 층은 적어도 2개의 매개변수를 받는다. 가능한 토큰의 개수(여기서는 1000으로 단어 인덱스 +1)와 임베딩 차원(여기서 64)\n",
    "    # Embedding 층은 (samples, sequence_length)인 2D 정수 텐서를 입력,\n",
    "    # (samples, sequence_length, embedding_dimensionality)인 3D 실수형 텐서 반환\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Embedding 층에 사용할 IMDB 데이터 로드\n",
    "\n",
    "max_features = 10000 # 특성으로 사용할 단어의 수\n",
    "maxlen = 20 # 사용할 텍스트의 길이(가장 빈번한 max_features개의 단어만 사용)\n",
    "\n",
    "imdb_data = tf.keras.datasets.imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb_data.load_data(num_words = max_features)\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen = maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen = maxlen)\n",
    "    # 리스트를 (samples, maxlen)크기의 2D 정수 텐서로 변환\n",
    "    # pad_sequence() 함수에는 패딩을 넣을 위치를 지정하는 padding 매개변수가 있다. 기본값 'pre'는 시퀀스 왼쪽에, 'post'는 오른쪽에 추가\n",
    "    # 여기서는 문장에서 20개 단어만 사용하므로 실제 패딩되는 샘플은 없다.\n",
    "    \n",
    "# IMDB 데이터에 Embedding 층과 분류기 사용하기\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length = maxlen)) # Flatten층 사용 위해 input_length 지정\n",
    "model.add(Flatten()) # 3D Embedding 텐서를 (samples, maxlen*8) 크기의 2D 텐서로 펼친다.\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['binary_accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 10, batch_size = 32, validation_split = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
